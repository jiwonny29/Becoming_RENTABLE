{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kT4oS7oyo8Rc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8qqZb2ysfGf",
        "outputId": "119c50d9-70b9-4ee0-e3f9-edc00f92385c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the custom training data from a CSV file in Google Drive with specific encoding\n",
        "custom_training_data = pd.read_csv('/content/drive/MyDrive/bert_custom_training_data.csv', encoding='ISO-8859-1')"
      ],
      "metadata": {
        "id": "jCpHwl7sswMR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count and display the frequency of each unique value in the 'sentiment' column\n",
        "custom_training_data['sentiment'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "VIDnHdtjsw_G",
        "outputId": "e96710d7-70d4-44b7-971c-feeafa4a35f3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentiment\n",
              "Neutral      1280\n",
              "Negative      450\n",
              "Positive      322\n",
              " positive       1\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sentiment</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Neutral</th>\n",
              "      <td>1280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Negative</th>\n",
              "      <td>450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Positive</th>\n",
              "      <td>322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter and sample the required number of rows for each sentiment\n",
        "positive_samples = custom_training_data[custom_training_data['sentiment'] == 'Positive'].sample(n=322, random_state=42)\n",
        "negative_samples = custom_training_data[custom_training_data['sentiment'] == 'Negative'].sample(n=340, random_state=42)\n",
        "neutral_samples = custom_training_data[custom_training_data['sentiment'] == 'Neutral'].sample(n=338, random_state=42)\n",
        "\n",
        "# Concatenate the samples into a single DataFrame\n",
        "custom_balanced_data = pd.concat([positive_samples, negative_samples, neutral_samples], ignore_index=True)\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "custom_balanced_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "0KRMuHaHs2w8",
        "outputId": "5c50676a-58eb-4b07-b52c-5020cff3b67f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       listing_id                                           comments  \\\n",
              "0    3.994421e+07  This house is extremely comfortable and beauti...   \n",
              "1    4.860920e+07  We loved this home.  The home was exactly what...   \n",
              "2    3.382268e+07  You are renting a floor in the house, not the ...   \n",
              "3    1.026391e+07  We really enjoyed how the place was set up. It...   \n",
              "4    4.992154e+17  My wife is handicap and it was very accessible...   \n",
              "..            ...                                                ...   \n",
              "995  4.893358e+07  Good location, nice spot for my sister, her hu...   \n",
              "996  1.064886e+07  Very spacious apartment. Quick walk to the con...   \n",
              "997  5.689689e+17  This place is wonderful! Perfect for 5 night s...   \n",
              "998  2.607599e+07  My team and I stayed here for our annual plann...   \n",
              "999  6.362317e+17  This place was the perfect find for our family...   \n",
              "\n",
              "                 keywords sentiment  \n",
              "0              wheelchair  Positive  \n",
              "1                 elderly  Positive  \n",
              "2              disability  Positive  \n",
              "3                disabled  Positive  \n",
              "4                handicap  Positive  \n",
              "..                    ...       ...  \n",
              "995              elevator   Neutral  \n",
              "996        step, spacious   Neutral  \n",
              "997          stair, large   Neutral  \n",
              "998  accessible, spacious   Neutral  \n",
              "999       stair, spacious   Neutral  \n",
              "\n",
              "[1000 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0d1310a9-5d8e-418a-a919-81a5d9579ca0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>listing_id</th>\n",
              "      <th>comments</th>\n",
              "      <th>keywords</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.994421e+07</td>\n",
              "      <td>This house is extremely comfortable and beauti...</td>\n",
              "      <td>wheelchair</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.860920e+07</td>\n",
              "      <td>We loved this home.  The home was exactly what...</td>\n",
              "      <td>elderly</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.382268e+07</td>\n",
              "      <td>You are renting a floor in the house, not the ...</td>\n",
              "      <td>disability</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.026391e+07</td>\n",
              "      <td>We really enjoyed how the place was set up. It...</td>\n",
              "      <td>disabled</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.992154e+17</td>\n",
              "      <td>My wife is handicap and it was very accessible...</td>\n",
              "      <td>handicap</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>4.893358e+07</td>\n",
              "      <td>Good location, nice spot for my sister, her hu...</td>\n",
              "      <td>elevator</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>1.064886e+07</td>\n",
              "      <td>Very spacious apartment. Quick walk to the con...</td>\n",
              "      <td>step, spacious</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>5.689689e+17</td>\n",
              "      <td>This place is wonderful! Perfect for 5 night s...</td>\n",
              "      <td>stair, large</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>2.607599e+07</td>\n",
              "      <td>My team and I stayed here for our annual plann...</td>\n",
              "      <td>accessible, spacious</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>6.362317e+17</td>\n",
              "      <td>This place was the perfect find for our family...</td>\n",
              "      <td>stair, spacious</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0d1310a9-5d8e-418a-a919-81a5d9579ca0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0d1310a9-5d8e-418a-a919-81a5d9579ca0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0d1310a9-5d8e-418a-a919-81a5d9579ca0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8c7ba8a8-9665-41bd-bdab-f94e5d91fdc7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8c7ba8a8-9665-41bd-bdab-f94e5d91fdc7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8c7ba8a8-9665-41bd-bdab-f94e5d91fdc7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_2b259247-8c16-436e-a519-ea8ad1f962a6\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('custom_balanced_data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_2b259247-8c16-436e-a519-ea8ad1f962a6 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('custom_balanced_data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "custom_balanced_data",
              "summary": "{\n  \"name\": \"custom_balanced_data\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"listing_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.1070021725494125e+17,\n        \"min\": 9596.0,\n        \"max\": 1.2140797781718433e+18,\n        \"num_unique_values\": 875,\n        \"samples\": [\n          8438090.0,\n          10125020.0,\n          24627763.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comments\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"This guesthouse is clean, cosy, quiet and safe. It is relatively easy to get to, but if you are a wheelchair user or have great difficulty with stairs, be aware that getting inside involves several steps and a narrow walkway. There is no bathroom sink, which, although not a great privation, was a bit of a surprise at first. The bed is comfortable and we didn't need the space heater at all with the warm fire going. All in all I would recommend Susan & Rob's place for singles or couples looking for a quiet place to relax at the end of a day of sightseeing.\",\n          \"This is a great place to stay in Austin.   The home is new, has a large living space downstairs and 3 comfortable bedrooms upstairs.   The pool and gazebo in the back is a great addition too.    The location is great with just a short drive to downtown Austin.   The host responded quickly with any questions that we had.  We would definitely stay here again.\",\n          \"We absolutely loved this booking!  it is easy to get to and in a great location!  It was clean and comfortable!  The 17th floor view was not of the lake, as in the picture.   That said I was entertained watching planes fly in, the beautiful morning, afternoon, and evening skyline.   All correspondence was prompt and helpful. <br/>I would note the following things for future guests to be aware of....<br/>1) I paid for parking at $40/day.  (which was steep i thought, but what do i know lol) The valets were friendly and prompt. <br/>2) The elevators during checkout/checkin hours were (as in many hotels) a bit of a hassle.   ie. One day we helped a lady holding her dog and a dog crate walk down 17 flights of stairs (due to waiting a crazy amount of time for the elevator to finally stop and repeatedly be full).  It is what it is, but plan accordingly.  :) <br/>3) It seemed there were a lot of fees.  ie $120 for cleaning fee.  <br/>Overall, one of my all time favorite rentals!!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"keywords\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 163,\n        \"samples\": [\n          \"accessible, tub\",\n          \"stair, disabled, accessible, spacious\",\n          \"elevator, large, big, tub\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Positive\",\n          \"Negative\",\n          \"Neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
        "import numpy as np\n",
        "\n",
        "# Label encoding\n",
        "label_encoder = LabelEncoder()\n",
        "custom_balanced_data['Sentiment'] = label_encoder.fit_transform(custom_balanced_data['sentiment'])\n",
        "\n",
        "# Splitting train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(custom_balanced_data['comments'], custom_balanced_data['Sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Creating datasets\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=512)\n",
        "test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=512)\n",
        "\n",
        "class ReviewsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = ReviewsDataset(train_encodings, y_train.tolist())\n",
        "test_dataset = ReviewsDataset(test_encodings, y_test.tolist())\n",
        "\n",
        "# Set weights based on class distribution\n",
        "class_counts = np.bincount(y_train)\n",
        "class_weights = 1. / class_counts\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "\n",
        "# Load RoBERTa model\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(label_encoder.classes_))\n",
        "\n",
        "# Custom Trainer class to include weighted loss\n",
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        # Compute loss with class weights\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights.to(logits.device))\n",
        "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# Set model training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        ")\n",
        "\n",
        "# Define evaluation metrics\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    precision = np.sum((preds == p.label_ids) & (preds == 1)) / np.sum(preds == 1)\n",
        "    recall = np.sum((preds == p.label_ids) & (preds == 1)) / np.sum(p.label_ids == 1)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n",
        "    accuracy = np.mean(preds == p.label_ids)\n",
        "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "# Trainer setup using WeightedTrainer\n",
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()  # Train model\n",
        "\n",
        "# Model evaluation\n",
        "results = trainer.evaluate()\n",
        "print(\"Evaluation Results:\", results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "IpabkGx_s6xs",
        "outputId": "9d4cbbe4-09ea-4597-a6fc-b947e08aa718"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [600/600 04:45, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.597700</td>\n",
              "      <td>0.440335</td>\n",
              "      <td>0.845000</td>\n",
              "      <td>0.855072</td>\n",
              "      <td>0.880597</td>\n",
              "      <td>0.867647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.590700</td>\n",
              "      <td>0.525876</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.921875</td>\n",
              "      <td>0.880597</td>\n",
              "      <td>0.900763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.310900</td>\n",
              "      <td>0.747765</td>\n",
              "      <td>0.830000</td>\n",
              "      <td>0.948276</td>\n",
              "      <td>0.820896</td>\n",
              "      <td>0.880000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results: {'eval_loss': 0.7477653622627258, 'eval_accuracy': 0.83, 'eval_precision': 0.9482758620689655, 'eval_recall': 0.8208955223880597, 'eval_f1': 0.8799999999999999, 'eval_runtime': 6.2823, 'eval_samples_per_second': 31.836, 'eval_steps_per_second': 7.959, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('content/drive/MyDrive/roberta_custom_saved_model')\n",
        "tokenizer.save_pretrained('content/drive/MyDrive/tokenizer_roberta_custom_saved_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH3__C2zumhj",
        "outputId": "bb7641f7-0e12-4ca1-849f-818cc29420df"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('content/drive/MyDrive/tokenizer_roberta_custom_saved_model/tokenizer_config.json',\n",
              " 'content/drive/MyDrive/tokenizer_roberta_custom_saved_model/special_tokens_map.json',\n",
              " 'content/drive/MyDrive/tokenizer_roberta_custom_saved_model/vocab.json',\n",
              " 'content/drive/MyDrive/tokenizer_roberta_custom_saved_model/merges.txt',\n",
              " 'content/drive/MyDrive/tokenizer_roberta_custom_saved_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM Model"
      ],
      "metadata": {
        "id": "RmmJaS2PGwie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load and balance dataset\n",
        "custom_training_data = pd.read_csv('/content/drive/MyDrive/bert_custom_training_data.csv', encoding='ISO-8859-1')\n",
        "positive_samples = custom_training_data[custom_training_data['sentiment'] == 'Positive'].sample(n=322, random_state=42)\n",
        "negative_samples = custom_training_data[custom_training_data['sentiment'] == 'Negative'].sample(n=340, random_state=42)\n",
        "neutral_samples = custom_training_data[custom_training_data['sentiment'] == 'Neutral'].sample(n=338, random_state=42)\n",
        "custom_balanced_data = pd.concat([positive_samples, negative_samples, neutral_samples], ignore_index=True)\n",
        "\n",
        "# Map sentiment to numerical values\n",
        "custom_balanced_data['sentiment'] = custom_balanced_data['sentiment'].map({'Positive': 2, 'Negative': 0, 'Neutral': 1})\n",
        "\n",
        "# Define hyperparameters\n",
        "class HyperParams:\n",
        "    PAD_INDEX = 0\n",
        "    UNK_INDEX = 1\n",
        "    PAD_TOKEN = '<pad>'\n",
        "    UNK_TOKEN = '<unk>'\n",
        "    STOP_WORDS = set(stopwords.words('english'))\n",
        "    MAX_LENGTH = 128\n",
        "    BATCH_SIZE = 32\n",
        "    EMBEDDING_DIM = 300  # Increased embedding dimension\n",
        "    HIDDEN_DIM = 256     # Increased hidden dimension\n",
        "    OUTPUT_DIM = 3       # Positive, Negative, Neutral\n",
        "    N_LAYERS = 3         # Increased LSTM layers\n",
        "    DROPOUT_RATE = 0.5   # Increased dropout for regularization\n",
        "    LR = 0.0003        # Lower learning rate\n",
        "    N_EPOCHS = 15        # More epochs\n",
        "    WD = 0\n",
        "    SEED = 42\n",
        "    BIDIRECTIONAL = True\n",
        "\n",
        "hparams = HyperParams()\n",
        "\n",
        "# Text Preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I | re.A).lower().strip()\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in hparams.STOP_WORDS]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "custom_balanced_data['comments'] = custom_balanced_data['comments'].apply(preprocess_text)\n",
        "\n",
        "# Split the data\n",
        "X = custom_balanced_data['comments'].values\n",
        "y = custom_balanced_data['sentiment'].values\n",
        "x_train, x_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=hparams.SEED, stratify=y)\n",
        "x_valid, x_test, y_valid, y_test = train_test_split(x_temp, y_temp, test_size=2/3, random_state=hparams.SEED, stratify=y_temp)\n",
        "\n",
        "# Build Vocabulary\n",
        "def build_vocab(x_train, min_freq=5):\n",
        "    word_counter = Counter()\n",
        "    for review in x_train:\n",
        "        word_counter.update(review.split())\n",
        "    vocab = {word: i+2 for i, word in enumerate([w for w, f in word_counter.items() if f >= min_freq])}\n",
        "    vocab[hparams.PAD_TOKEN] = hparams.PAD_INDEX\n",
        "    vocab[hparams.UNK_TOKEN] = hparams.UNK_INDEX\n",
        "    return vocab\n",
        "\n",
        "vocab = build_vocab(x_train)\n",
        "\n",
        "# Tokenize\n",
        "def tokenize(vocab, text):\n",
        "    return [vocab.get(word, vocab['<unk>']) for word in text.split()]\n",
        "\n",
        "# Dataset Class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, x, y, vocab):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text, label = self.x[idx], self.y[idx]\n",
        "        tokenized_text = tokenize(self.vocab, text)\n",
        "        if len(tokenized_text) > hparams.MAX_LENGTH:\n",
        "            tokenized_text = tokenized_text[:hparams.MAX_LENGTH]\n",
        "        else:\n",
        "            tokenized_text += [self.vocab[hparams.PAD_TOKEN]] * (hparams.MAX_LENGTH - len(tokenized_text))\n",
        "        return torch.tensor(tokenized_text), torch.tensor(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "# Collate Function\n",
        "def collate_fn(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    texts = torch.stack(texts)\n",
        "    labels = torch.stack(labels)\n",
        "    return texts, labels\n",
        "\n",
        "# LSTM Model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout_rate, pad_index, bidirectional):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout_rate, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
        "        if self.lstm.bidirectional:\n",
        "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "        else:\n",
        "            hidden = hidden[-1,:,:]\n",
        "        return self.fc(hidden)\n",
        "\n",
        "# Prepare Datasets and DataLoaders\n",
        "train_dataset = CustomDataset(x_train, y_train, vocab)\n",
        "valid_dataset = CustomDataset(x_valid, y_valid, vocab)\n",
        "test_dataset = CustomDataset(x_test, y_test, vocab)\n",
        "train_loader = DataLoader(train_dataset, batch_size=hparams.BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=hparams.BATCH_SIZE, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=hparams.BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "# Initialize Model, Optimizer, and Loss Function\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LSTMModel(len(vocab), hparams.EMBEDDING_DIM, hparams.HIDDEN_DIM, hparams.OUTPUT_DIM, hparams.N_LAYERS, hparams.DROPOUT_RATE, hparams.PAD_INDEX, hparams.BIDIRECTIONAL).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=hparams.LR)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training and Evaluation\n",
        "def train(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    epoch_loss, epoch_acc = 0, 0\n",
        "    for texts, labels in loader:\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(texts)\n",
        "        loss = criterion(predictions, labels)\n",
        "        acc = (predictions.argmax(1) == labels).sum().item() / len(labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc\n",
        "    return epoch_loss / len(loader), epoch_acc / len(loader)\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss, epoch_acc = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            predictions = model(texts)\n",
        "            loss = criterion(predictions, labels)\n",
        "            acc = (predictions.argmax(1) == labels).sum().item() / len(labels)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc\n",
        "    return epoch_loss / len(loader), epoch_acc / len(loader)\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(hparams.N_EPOCHS):\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_loader, criterion)\n",
        "    print(f\"Epoch {epoch+1}/{hparams.N_EPOCHS}\")\n",
        "    print(f\"Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%\")\n",
        "    print(f\"Valid Loss: {valid_loss:.3f}, Valid Acc: {valid_acc*100:.2f}%\")\n",
        "\n",
        "# Test Evaluation\n",
        "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
        "print(f\"Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYsK3FHMGzFo",
        "outputId": "f8bcf1c0-f89a-4ddc-cf5c-dee9653ee1f2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Epoch 1/15\n",
            "Train Loss: 1.099, Train Acc: 30.93%\n",
            "Valid Loss: 1.098, Valid Acc: 35.94%\n",
            "Epoch 2/15\n",
            "Train Loss: 1.093, Train Acc: 41.44%\n",
            "Valid Loss: 1.098, Valid Acc: 30.47%\n",
            "Epoch 3/15\n",
            "Train Loss: 1.069, Train Acc: 48.07%\n",
            "Valid Loss: 1.066, Valid Acc: 33.59%\n",
            "Epoch 4/15\n",
            "Train Loss: 0.988, Train Acc: 55.95%\n",
            "Valid Loss: 1.076, Valid Acc: 43.75%\n",
            "Epoch 5/15\n",
            "Train Loss: 0.898, Train Acc: 58.44%\n",
            "Valid Loss: 0.932, Valid Acc: 57.81%\n",
            "Epoch 6/15\n",
            "Train Loss: 0.795, Train Acc: 65.46%\n",
            "Valid Loss: 1.022, Valid Acc: 48.44%\n",
            "Epoch 7/15\n",
            "Train Loss: 0.736, Train Acc: 68.71%\n",
            "Valid Loss: 0.950, Valid Acc: 53.91%\n",
            "Epoch 8/15\n",
            "Train Loss: 0.686, Train Acc: 69.40%\n",
            "Valid Loss: 1.003, Valid Acc: 56.25%\n",
            "Epoch 9/15\n",
            "Train Loss: 0.640, Train Acc: 73.15%\n",
            "Valid Loss: 1.250, Valid Acc: 42.97%\n",
            "Epoch 10/15\n",
            "Train Loss: 0.648, Train Acc: 71.31%\n",
            "Valid Loss: 1.037, Valid Acc: 53.12%\n",
            "Epoch 11/15\n",
            "Train Loss: 0.527, Train Acc: 79.71%\n",
            "Valid Loss: 1.025, Valid Acc: 53.91%\n",
            "Epoch 12/15\n",
            "Train Loss: 0.500, Train Acc: 80.70%\n",
            "Valid Loss: 1.201, Valid Acc: 57.03%\n",
            "Epoch 13/15\n",
            "Train Loss: 0.383, Train Acc: 84.29%\n",
            "Valid Loss: 1.478, Valid Acc: 46.09%\n",
            "Epoch 14/15\n",
            "Train Loss: 0.392, Train Acc: 84.72%\n",
            "Valid Loss: 1.272, Valid Acc: 52.34%\n",
            "Epoch 15/15\n",
            "Train Loss: 0.362, Train Acc: 85.25%\n",
            "Valid Loss: 1.108, Valid Acc: 66.41%\n",
            "Test Loss: 1.266, Test Acc: 51.34%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT2 Model"
      ],
      "metadata": {
        "id": "8KUoBrB30XCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers datasets peft\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import AutoTokenizer, GPT2Model, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0jNAZnf4sD4",
        "outputId": "a3b1d602-81e7-4f9c-8831-fe79471337f1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.0+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.34.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n45Uuu7G4zYL",
        "outputId": "dc01e04b-8d27-4b65-aee7-c3161281adbc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    custom_balanced_data['comments'].values, custom_balanced_data['sentiment'].values,\n",
        "    test_size=0.3, random_state=42, stratify=custom_balanced_data['sentiment']\n",
        ")\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "# Load GPT-2 tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS token for compatibility\n",
        "\n",
        "# Initialize the GPT-2 model for sequence classification\n",
        "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=3)\n",
        "model.config.pad_token_id = tokenizer.eos_token_id  # Set padding token ID in model configuration\n",
        "\n",
        "# Create a custom Dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt'\n",
        "        )\n",
        "        input_ids = encoding['input_ids'].squeeze()  # Remove extra batch dimension\n",
        "        attention_mask = encoding['attention_mask'].squeeze()  # Remove extra batch dimension\n",
        "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': torch.tensor(label)}\n",
        "\n",
        "# Prepare datasets\n",
        "train_dataset = CustomDataset(X_train, y_train, tokenizer)\n",
        "valid_dataset = CustomDataset(X_valid, y_valid, tokenizer)\n",
        "test_dataset = CustomDataset(X_test, y_test, tokenizer)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# Define a function to compute accuracy\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    accuracy = (preds == labels).mean()\n",
        "    return {\"accuracy\": accuracy}\n",
        "\n",
        "# Set up Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "print(\"Test Results:\", test_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "3eh_AmAvEEMy",
        "outputId": "4940167c-66ed-47f1-a19b-322759c9e10d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='525' max='525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [525/525 02:01, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.228500</td>\n",
              "      <td>1.030749</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.521500</td>\n",
              "      <td>0.695812</td>\n",
              "      <td>0.713333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.847400</td>\n",
              "      <td>0.913883</td>\n",
              "      <td>0.740000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [38/38 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results: {'eval_loss': 0.9189814329147339, 'eval_accuracy': 0.7066666666666667, 'eval_runtime': 1.2822, 'eval_samples_per_second': 116.985, 'eval_steps_per_second': 29.636, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kLlRiXUELT2l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}